{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import bz2\nfrom collections import Counter\nimport re\nimport nltk\nimport numpy as np\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:03:42.694579Z","iopub.execute_input":"2023-05-07T07:03:42.694913Z","iopub.status.idle":"2023-05-07T07:03:43.804871Z","shell.execute_reply.started":"2023-05-07T07:03:42.694885Z","shell.execute_reply":"2023-05-07T07:03:43.804035Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"train_file = bz2.BZ2File('../input/amazonreviews/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('../input/amazonreviews/test.ft.txt.bz2')\n\ntrain_file = train_file.readlines()\ntest_file = test_file.readlines()","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:03:43.806600Z","iopub.execute_input":"2023-05-07T07:03:43.807010Z","iopub.status.idle":"2023-05-07T07:05:30.210362Z","shell.execute_reply.started":"2023-05-07T07:03:43.806977Z","shell.execute_reply":"2023-05-07T07:05:30.209461Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Reducing size of the dataset","metadata":{}},{"cell_type":"code","source":"num_train = 800000  # We're training on the first 800,000 reviews in the dataset\nnum_test = 200000  # Using 200,000 reviews from test set\n\ntrain_file = [x.decode('utf-8') for x in train_file[:num_train]]\ntest_file = [x.decode('utf-8') for x in test_file[:num_test]]\n\nprint(train_file[0])\nprint(test_file[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:05:30.211890Z","iopub.execute_input":"2023-05-07T07:05:30.212230Z","iopub.status.idle":"2023-05-07T07:05:31.181140Z","shell.execute_reply.started":"2023-05-07T07:05:30.212197Z","shell.execute_reply":"2023-05-07T07:05:31.180163Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n\n__label__2 Great CD: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# Extracting labels\ntrain_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\nprint(train_labels[:5])\nprint(train_sentences[0])\n\ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:05:31.183684Z","iopub.execute_input":"2023-05-07T07:05:31.184279Z","iopub.status.idle":"2023-05-07T07:05:35.609448Z","shell.execute_reply.started":"2023-05-07T07:05:31.184228Z","shell.execute_reply":"2023-05-07T07:05:35.608441Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[1, 1, 1, 1, 1]\nstuning even for the non-gamer: this sound track was beautiful! it paints the senery in your mind so well i would recomend it even to people who hate vid. game music! i have played the game chrono cross but out of all of the games i have ever played it has the best music! it backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. it would impress anyone who cares to listen! ^_^\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n\nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n\n# Modify URLs to <url>\nfor i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:05:35.610681Z","iopub.execute_input":"2023-05-07T07:05:35.611024Z","iopub.status.idle":"2023-05-07T07:05:42.530117Z","shell.execute_reply.started":"2023-05-07T07:05:35.610995Z","shell.execute_reply":"2023-05-07T07:05:42.529234Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"words = Counter()\nfor i, sentence in enumerate(train_sentences):\n    train_sentences[i] = []\n    \n    for word in nltk.word_tokenize(sentence):\n        # using list because else Counter will add letetrs, not whole words\n        words.update([word.lower()])\n        train_sentences[i].append(word)\n        \n    if i % 20000 == 0:\n        print(str((i*100)/num_train) + '% done.')\n\nprint('100% done.')","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:05:42.531323Z","iopub.execute_input":"2023-05-07T07:05:42.531671Z","iopub.status.idle":"2023-05-07T07:16:09.148921Z","shell.execute_reply.started":"2023-05-07T07:05:42.531641Z","shell.execute_reply":"2023-05-07T07:16:09.147923Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"0.0% done.\n2.5% done.\n5.0% done.\n7.5% done.\n10.0% done.\n12.5% done.\n15.0% done.\n17.5% done.\n20.0% done.\n22.5% done.\n25.0% done.\n27.5% done.\n30.0% done.\n32.5% done.\n35.0% done.\n37.5% done.\n40.0% done.\n42.5% done.\n45.0% done.\n47.5% done.\n50.0% done.\n52.5% done.\n55.0% done.\n57.5% done.\n60.0% done.\n62.5% done.\n65.0% done.\n67.5% done.\n70.0% done.\n72.5% done.\n75.0% done.\n77.5% done.\n80.0% done.\n82.5% done.\n85.0% done.\n87.5% done.\n90.0% done.\n92.5% done.\n95.0% done.\n97.5% done.\n100% done.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Building vocab and maps for words and respective ids","metadata":{}},{"cell_type":"code","source":"# Removing the words that only appear once\nwords = {k:v for k, v in words.items() if v>1}\n\n# Sorting the words according to the number of appearances, with the most common word being first\nwords = sorted(words, key=words.get, reverse=True)\n\n# Adding padding and unknown to our vocabulary so that they will be assigned an index\nwords = ['_PAD','_UNK'] + words\n\n# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:16:09.150233Z","iopub.execute_input":"2023-05-07T07:16:09.151707Z","iopub.status.idle":"2023-05-07T07:16:09.576343Z","shell.execute_reply.started":"2023-05-07T07:16:09.151669Z","shell.execute_reply":"2023-05-07T07:16:09.574654Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(list(word2idx.items())[:10])\nprint(list(idx2word.items())[:10])\nprint(len(idx2word))","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:16:09.577866Z","iopub.execute_input":"2023-05-07T07:16:09.578232Z","iopub.status.idle":"2023-05-07T07:16:09.777149Z","shell.execute_reply.started":"2023-05-07T07:16:09.578199Z","shell.execute_reply":"2023-05-07T07:16:09.776168Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[('_PAD', 0), ('_UNK', 1), ('.', 2), ('the', 3), (',', 4), ('i', 5), ('and', 6), ('a', 7), ('to', 8), ('it', 9)]\n[(0, '_PAD'), (1, '_UNK'), (2, '.'), (3, 'the'), (4, ','), (5, 'i'), (6, 'and'), (7, 'a'), (8, 'to'), (9, 'it')]\n225964\n","output_type":"stream"}]},{"cell_type":"code","source":"for i, sentence in enumerate(train_sentences):\n    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n\nfor i, sentence in enumerate(test_sentences):\n    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:16:09.778496Z","iopub.execute_input":"2023-05-07T07:16:09.779341Z","iopub.status.idle":"2023-05-07T07:18:46.655797Z","shell.execute_reply.started":"2023-05-07T07:16:09.779306Z","shell.execute_reply":"2023-05-07T07:18:46.654872Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Padding sentences","metadata":{}},{"cell_type":"code","source":"# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\nseq_len = 200\n\ntrain_sentences = pad_input(train_sentences, seq_len)\ntest_sentences = pad_input(test_sentences, seq_len)\n\n# Converting our labels into numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:18:46.659097Z","iopub.execute_input":"2023-05-07T07:18:46.659558Z","iopub.status.idle":"2023-05-07T07:18:56.274171Z","shell.execute_reply.started":"2023-05-07T07:18:46.659520Z","shell.execute_reply":"2023-05-07T07:18:56.273204Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(train_sentences[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:18:56.275523Z","iopub.execute_input":"2023-05-07T07:18:56.275869Z","iopub.status.idle":"2023-05-07T07:18:56.283737Z","shell.execute_reply.started":"2023-05-07T07:18:56.275838Z","shell.execute_reply":"2023-05-07T07:18:56.282531Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[     0      0      0      0      0      0      0      0      0      0\n      0      0      0      0      0      0      0      0      0      0\n      0      0      0      0      0      0      0      0      0      0\n      0      0      0      0      0      0      0      0      0      0\n      0      0      0      0      0      0      0      0      0      0\n      0      0      0      0      0      0      0      0      0      0\n      0      0      0      0      0      0      0      0      0      0\n      0      0      0      0      0      0      0      0      0      0\n      0      0      0      0      0      0      0      0      0      0\n      0      0      0      0      0      0      0      0      0      0\n      0      0      0      0      0      0      0      0      0      0\n      0      0      0  66192     88     16      3 103189     13     11\n    192    458     18    361     15      9   5735      3  91126     14\n     77    433     36     90      5     51   1659      9     88      8\n    141     80    616  18524      2    211    126     15      5     27\n    539      3    211  18204   2030     22     56     10     35     10\n      3    792      5     27    131    539      9     58      3     96\n    126     15      9   7389    261     49   4577  62468      6    427\n      7  17668   1077     23   8890   2706      6   3930  19354      2\n      9     51   4797    204     80   2390      8    313     15  17004]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Splitting data into test and validation","metadata":{}},{"cell_type":"code","source":"split_frac = 0.5 # 50% validation, 50% test\nsplit_id = int(split_frac * len(test_sentences))\nval_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\nval_labels, test_labels = test_labels[:split_id], test_labels[split_id:]","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:18:56.285756Z","iopub.execute_input":"2023-05-07T07:18:56.287101Z","iopub.status.idle":"2023-05-07T07:18:56.293461Z","shell.execute_reply.started":"2023-05-07T07:18:56.287053Z","shell.execute_reply":"2023-05-07T07:18:56.292464Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Building model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\n\ntrain_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\nval_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\ntest_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n\nbatch_size = 400\n\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nval_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:18:56.294862Z","iopub.execute_input":"2023-05-07T07:18:56.297667Z","iopub.status.idle":"2023-05-07T07:18:57.723911Z","shell.execute_reply.started":"2023-05-07T07:18:56.297527Z","shell.execute_reply":"2023-05-07T07:18:57.722846Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Checking if we can use CPU or GPU\nis_cuda = torch.cuda.is_available()\n\nif is_cuda:\n  device = torch.device('cuda')\n  print('GPU is available')\nelse:\n  device = torch.device('cpu')\n  print('GPU is not available, CPU is used')","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:18:57.725526Z","iopub.execute_input":"2023-05-07T07:18:57.726180Z","iopub.status.idle":"2023-05-07T07:18:57.802615Z","shell.execute_reply.started":"2023-05-07T07:18:57.726132Z","shell.execute_reply":"2023-05-07T07:18:57.801489Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"GPU is available\n","output_type":"stream"}]},{"cell_type":"code","source":"class SentimentNet(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        super(SentimentNet, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        out = self.sigmoid(out)\n        \n        out = out.view(batch_size, -1)\n        out = out[:,-1]\n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        print('weight:', weight.shape)\n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n        print('hidden:', hidden[0].shape)\n        return hidden","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:18:57.804302Z","iopub.execute_input":"2023-05-07T07:18:57.805950Z","iopub.status.idle":"2023-05-07T07:18:57.817094Z","shell.execute_reply.started":"2023-05-07T07:18:57.805913Z","shell.execute_reply":"2023-05-07T07:18:57.815988Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(word2idx) + 1\noutput_size = 1\nembedding_dim = 400\nhidden_dim = 512\nn_layers = 2\n\nmodel = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nmodel.to(device)\n\nlr=0.005\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:18:57.818725Z","iopub.execute_input":"2023-05-07T07:18:57.819600Z","iopub.status.idle":"2023-05-07T07:19:02.663810Z","shell.execute_reply.started":"2023-05-07T07:18:57.819563Z","shell.execute_reply":"2023-05-07T07:19:02.662881Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"epochs = 2\ncounter = 0\nprint_every = 1000\nclip = 5\nvalid_loss_min = np.Inf\n\nmodel.train()\nfor i in range(epochs):\n    h = model.init_hidden(batch_size)\n    \n    for inputs, labels in train_loader:\n        counter += 1\n        h = tuple([e.data for e in h])\n        inputs, labels = inputs.to(device), labels.to(device)\n        model.zero_grad()\n        output, h = model(inputs, h)\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        if counter%print_every == 0:\n            val_h = model.init_hidden(batch_size)\n            val_losses = []\n            model.eval()\n            for inp, lab in val_loader:\n                val_h = tuple([each.data for each in val_h])\n#                 print('val_h:', val_h[0].shape)\n                inp, lab = inp.to(device), lab.to(device)\n                out, val_h = model(inp, val_h)\n                val_loss = criterion(out.squeeze(), lab.float())\n                val_losses.append(val_loss.item())\n                \n            model.train()\n            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n            if np.mean(val_losses) <= valid_loss_min:\n                torch.save(model.state_dict(), './state_dict.pt')\n                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n                valid_loss_min = np.mean(val_losses)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:19:02.665008Z","iopub.execute_input":"2023-05-07T07:19:02.665330Z","iopub.status.idle":"2023-05-07T08:20:03.057753Z","shell.execute_reply.started":"2023-05-07T07:19:02.665301Z","shell.execute_reply":"2023-05-07T08:20:03.056834Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"weight: torch.Size([225965, 400])\nhidden: torch.Size([2, 400, 512])\nweight: torch.Size([225965, 400])\nhidden: torch.Size([2, 400, 512])\nEpoch: 1/2... Step: 1000... Loss: 0.140310... Val Loss: 0.177986\nValidation loss decreased (inf --> 0.177986).  Saving model ...\nweight: torch.Size([225965, 400])\nhidden: torch.Size([2, 400, 512])\nEpoch: 1/2... Step: 2000... Loss: 0.161809... Val Loss: 0.175755\nValidation loss decreased (0.177986 --> 0.175755).  Saving model ...\nweight: torch.Size([225965, 400])\nhidden: torch.Size([2, 400, 512])\nweight: torch.Size([225965, 400])\nhidden: torch.Size([2, 400, 512])\nEpoch: 2/2... Step: 3000... Loss: 0.208834... Val Loss: 0.176947\nweight: torch.Size([225965, 400])\nhidden: torch.Size([2, 400, 512])\nEpoch: 2/2... Step: 4000... Loss: 0.179017... Val Loss: 0.178003\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading model","metadata":{}},{"cell_type":"code","source":"# Loading the best model\nmodel.load_state_dict(torch.load('./state_dict.pt'))\n\ntest_losses = []\nnum_correct = 0\nh = model.init_hidden(batch_size)\n\nmodel.eval()\nfor inputs, labels in test_loader:\n    h = tuple([each.data for each in h])\n    inputs, labels = inputs.to(device), labels.to(device)\n    output, h = model(inputs, h)\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\ntest_acc = num_correct/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}%\".format(test_acc*100))","metadata":{"execution":{"iopub.status.busy":"2023-05-07T08:27:31.809064Z","iopub.execute_input":"2023-05-07T08:27:31.809526Z","iopub.status.idle":"2023-05-07T08:28:39.331246Z","shell.execute_reply.started":"2023-05-07T08:27:31.809488Z","shell.execute_reply":"2023-05-07T08:28:39.330298Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"weight: torch.Size([225965, 400])\nhidden: torch.Size([2, 400, 512])\nTest loss: 0.168\nTest accuracy: 93.595%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}